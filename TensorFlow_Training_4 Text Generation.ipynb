{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "olSbQlWGmaOU"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_CZy5s5rsK9"
   },
   "source": [
    "### 1. Load Shakespeare book\n",
    "\n",
    "It is a large corpus of text, it is highly recommended to have at least one source of a million characters to get a realistic text generation.\n",
    "\n",
    "The text is well structured. It's in form of:\n",
    "\n",
    "$Character_X$: $Speech_X$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_Rxuga-KmvEx"
   },
   "outputs": [],
   "source": [
    "with open('shakespeare.txt', 'r') as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "95HstfI5x-HM",
    "outputId": "36d8cc22-f041-4199-a114-caa99418f584"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose mi\""
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCorj_nf0GDk"
   },
   "source": [
    "### 2. Preprocessing\n",
    "\n",
    "- Remove big spaces\n",
    "- Word2vect: assign numbers to each character\n",
    "  - Ceate first dict that can switch between a numerical index to a character \n",
    "  - Create second dict and a character to a numerical index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-z5qGgZFBVi"
   },
   "source": [
    "#### 2.1 Remove big spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sNgCoUYHx_qb",
    "outputId": "70ee5f09-33eb-431e-aae8-54c20edc3ee8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose mi\""
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = re.sub(r\"\\s+\",\" \", text)\n",
    "\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwYaFHcVE7vc"
   },
   "source": [
    "##### 2.2 Create a mapping dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_pfyAExyqEd0",
    "outputId": "01be85c9-3244-43da-98de-2be7f6638ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 ['\\n', ' ', '!', '\"', '&', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "# Sorted and unique set of characters\n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(len(vocab), vocab[:6])\n",
    "\n",
    "char_to_index = {character: nb for nb, character in enumerate(vocab)}\n",
    "index_to_char = np.array(vocab) # dict(map(reversed, char_to_index.items())) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMJry2qF8ape"
   },
   "source": [
    "#### 2.3 Encode the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NkztSc0xuEXb",
    "outputId": "6d4619da-58a5-4ce2-b952-df2bab63b344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time\n",
      "\n",
      "Encoded text: \n",
      "[59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63 56 75  1\n",
      " 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60  1 68 64\n",
      " 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1 56 74  1\n",
      " 75 63 60  1 73 64 71 60 73  1 74 63 70 76 67 59  1 57 80  1 75 64 68 60]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_text = np.array([char_to_index[char] for char in text])\n",
    "\n",
    "start, end = 52, 148\n",
    "print(f\"Original text:\\n{text[start: end]}\\n\")\n",
    "print(f\"Encoded text: \\n{encoded_text[start: end]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7Th2s_k5KEW"
   },
   "source": [
    "## 3. Create batchs\n",
    "\n",
    "1. Find a proper lenght for the sequence\n",
    "  - Too short a sequence: we will not have enough information \n",
    "  - Too long a sequence and the training will take too long and risk over-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5DlT6GLFypC"
   },
   "source": [
    "3.1 Lenght for the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Rn7gs8DwmU3",
    "outputId": "a9077e5c-6209-4387-d108-79de05be9942"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 45005)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 120 # int(np.mean([len(sentence) for sentence in text.split(',')])) * 3 # Around 3 sentences\n",
    "total_num_seq = len(text) // (seq_len + 1) # 1: Because the index starts from \n",
    "\n",
    "seq_len, total_num_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sbvj4WrHN-J"
   },
   "source": [
    "3.2 Creating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "__OwbtR9G2gC"
   },
   "outputs": [],
   "source": [
    "batch_size, buffer_size = 128, 10000\n",
    "# buffer_size: shuffle 10000 elements in your dataset\n",
    "# useful for memory when we are dealing with big dataset\n",
    "\n",
    "def create_seq_targets(seq):\n",
    "  \"\"\"\n",
    "  seq: Hello my name is Celia\n",
    "  input_txt: Hello my name is Celi\n",
    "  target_txt: ello my name is Celia\n",
    "  \"\"\"\n",
    "  input_txt = seq[:-1]\n",
    "  target_txt = seq[1:]\n",
    "\n",
    "  return input_txt, target_txt\n",
    "\n",
    "def display_xy(dataset):\n",
    "\n",
    "  for X, y in dataset.take(1):\n",
    "    print(f\"Input_X:\\n{X}\\n\")\n",
    "    print(f\"type(Input_X):{type(X)}\\n\")\n",
    "    print(f\"Input_X:\\n{''.join(index_to_char[X.numpy()])}\\n\\n\")\n",
    "\n",
    "    print(f\"Output_y:\\n{y}\\n\")\n",
    "    print(f\"type(Output_y):{type(y)}\\n\")\n",
    "    print(f\"Output_y:\\n{''.join(index_to_char[y.numpy()])}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bv5a6LVb50K-",
    "outputId": "abb219d8-36c5-451a-b51f-48256c582d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(char_dataset): <class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Creating training sequences\n",
    "\n",
    "# Convertir un vecteur de texte en un flux d'indices de caractÃ¨res.\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "\n",
    "print(f\"type(char_dataset): {type(char_dataset)}\")\n",
    "\n",
    "for index, item in enumerate(char_dataset.take(500)):\n",
    "  print(item.numpy())\n",
    "  if index > 2: \n",
    "    break\n",
    "    \n",
    "# drop_remainder: drop_remainder: (Optional.) \n",
    "# A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements;\n",
    "# the default behavior is not to drop the smaller batch.\n",
    "sequences = char_dataset.batch(seq_len + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIww79ldHvoR"
   },
   "source": [
    "3.3 Creating input seq and target output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LUku_TpD-g_3"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(create_seq_targets)\n",
    "\n",
    "#display_xy(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PetlIerdICoR"
   },
   "source": [
    "3.4 Create the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NVCKxNTICvQ",
    "outputId": "a8bcc2df-6c1d-4f35-bb2b-bf26bcd6d2fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 120), (128, 120)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, \n",
    "                                             drop_remainder=True)\n",
    "\n",
    "# tuple 1: input (batch_size, seq_length)\n",
    "# tuple 2: target (batch_size, seq_length)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-eITRCHCWeJ"
   },
   "source": [
    "## 4. Creating the model\n",
    "\n",
    "Use sparse categorical crossentropy when your classes are mutually exclusive (e.g. when each sample belongs exactly to one class) and categorical crossentropy when one sample can have multiple classes or labels are soft probabilities (like [0.5, 0.3, 0.2])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DznJpV-A-uec",
    "outputId": "5bb6dd7c-1f16-4b43-d529-71a281569f60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The vocab is the number of unique words in the vocabulary\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Size of embedding\n",
    "embed_dim = 64 # ~ < vocab_size\n",
    "\n",
    "# Size of RNN\n",
    "rnn_neurons = 1026\n",
    "\n",
    "# Epochs\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9slNkzJZk2V"
   },
   "source": [
    "#### PARAMETERS OF THE EMBEDDING LAYER ---\n",
    "\n",
    "**input_dim** = the vocab size that we will choose. In other words it is the number of unique words in the vocab.\n",
    "\n",
    "**output_dim** = the number of dimensions we wish to embed into. Each word will be represented by a vector of this much dimensions.\n",
    "\n",
    "**input_length** = lenght of the maximum document. which can be stored in max_len\n",
    "\n",
    "In our case:\n",
    "\n",
    "`dataset = <BatchDataset shapes: ((128, 116), (128, 116)), types: (tf.int64, tf.int64)>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "62gFvQ8ZA2XQ"
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embed_dim, batch_size):\n",
    "  \n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, embed_dim, batch_input_shape= [batch_size, None]))\n",
    "  model.add(GRU(rnn_neurons,return_sequences=True, \n",
    "                stateful=True,\n",
    "                recurrent_initializer='glorot_uniform'))\n",
    "  # Couche Finale Dense de PrÃ©diction\n",
    "  model.add(Dense(vocab_size))\n",
    "  model.compile(optimizer='adam', loss=lambda y_true, y_pred: sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)) \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjXy7zlTa5kI",
    "outputId": "911c0f11-d3b6-4e4f-823e-8d6bb31dace7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 64)           5376      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (128, None, 1026)         3361176   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 84)           86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, embed_dim, batch_size)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAN2AaJfxOS-"
   },
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOaMr0I2a-SG",
    "outputId": "cc01a9c9-5d2c-4b9c-e49b-b899391e226f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 120, 84)  <=== (batch_size, sequence_length, vocab_size)\n",
      "(120, 84)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "\n",
    "  # PrÃ©dire sur un lot alÃ©atoire\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "\n",
    "  # Afficher les dimensions des prÃ©dictions\n",
    "  print(example_batch_predictions.shape, \" <=== (batch_size, sequence_length, vocab_size)\")\n",
    "  # Probability for each char\n",
    "  print(example_batch_predictions[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "CZxINHAtcgW3"
   },
   "outputs": [],
   "source": [
    "# Prendre un Ã©chantillon alÃ©atoirement avec une log proba\n",
    "\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "\n",
    "# squeeze: Removes dimensions of size 1 from the shape of a tensor.\n",
    "alea_char = index_to_char[tf.squeeze(sampled_indices, axis=1).numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uiqs7nxvvxpe"
   },
   "outputs": [],
   "source": [
    "# model.fit(dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Rx_-6Gm_-yW6"
   },
   "outputs": [],
   "source": [
    "# Save the model \n",
    "# model.save(\"shakespeare_gen.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zGDzH55Kg09M",
    "outputId": "cd5c136b-b775-48ad-c787-5de5fbee17d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 64)             5376      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1026)           3361176   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 84)             86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "\n",
    "model2 = create_model(vocab_size, embed_dim, batch_size=1) \n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emILSBFayO3t",
    "outputId": "ccda3661-0ba2-4640-a0c8-bb251c457d7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 64)             5376      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1026)           3361176   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 84)             86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.load_weights('shakespeare_gen.h5')\n",
    "model2.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "M3-2KQpzwYWO"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, \n",
    "                  start_text, \n",
    "                  generate_size=500, \n",
    "                  temperature=1.0):\n",
    "  \n",
    "  text_generated = []\n",
    "\n",
    "  input = [char_to_index[s] for s in start_text]\n",
    "\n",
    "  # To fit with the batch size, cause we are using batch of size 1\n",
    "  input = tf.expand_dims(input, 0)\n",
    "\n",
    "\n",
    "  model.reset_states()\n",
    "\n",
    "  for t in range(generate_size):\n",
    "    predictions = model(input)\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "    \n",
    "    predictions = predictions / temperature\n",
    "\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "    input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    text_generated.append(index_to_char[predicted_id])\n",
    "\n",
    "  return f\"{start_text}{''.join(text_generated)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "uA9jXt4-yVSN",
    "outputId": "dd157c34-962f-4231-bb70-bcc15407f1fd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"JULIETTIMER. There's old Nor God-here purpose; and what try\\n    For this exceptance gave away myself, I\\n  \""
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model2, 'JULIET', generate_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "od5hNft5yVoD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tensorflow NLP - Text generation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
